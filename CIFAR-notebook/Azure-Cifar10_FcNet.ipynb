{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# CIFAR 10 Classification using Neural networks\n",
    "\n",
    "##### Training a fully connected network on images and improving its classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "\n",
    "* [Introduction](#intro)\n",
    "* [Setup](#setup)\n",
    "    * [Import libraries](#lib)\n",
    "    * [Configure the Workspace](#ws)\n",
    "    * [Create the Experiment](#exp)\n",
    "    * [Utilize the Compute Resources](#compute)\n",
    "    * [Upload Dataset](#dataset)\n",
    "* [Train a baseline model](#train)\n",
    "* [Perform Automated Hyperparameter Tuning](#hyper)\n",
    "* [Results](#results)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction <a class=\"anchor\" id=\"intro\"></a> \n",
    "\n",
    "CIFAR 10 is a popular image classification dataset. It provides labeled images across different classes. The challenge is to correctly classify them in their respective classes. More information about the dataset can be found at their website -  https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "Neural networks are the state of the art in most of the computer vision challenges. In this tutorial, we present detailed steps to create a simple network for the classification task. \n",
    "\n",
    "The entire notebook has been created in Python. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "We walk through the sequence of steps to put everything in place for our task. Further details about each step of the process can be found here - https://docs.microsoft.com/en-us/azure/machine-learning/service/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries <a class=\"anchor\" id=\"lib\"></a>\n",
    "\n",
    "We start by importing some python libraries for performing the classification task. Numpy provides support for performing scientific computation. Matplotlib is used to display the plots created during the task. \n",
    "\n",
    "Apart from them, we also import the libraries provided by Azure Machine learning. The core library from AzureML helps us lay the foundations for carrying out the task on the Azure platform. We import *Workspace* in the below step and display the Azure ML SDK version installed on our machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.0.10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Pytorch libraries provided with the Azure platform\n",
    "\n",
    "In this tutorial, we will create a script using Pytorch. Azure Machine learning platform supports running pytorch scripts. To do the same, we import the corresponding module from the Deep Neural Network library provided by Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the workspace <a class=\"anchor\" id=\"ws\"></a>\n",
    "\n",
    "Once we have all the libraries with us, the first step we take is to load the workspace where our actual experiment will happen. A workspace is always associated with an Azure subscription. This workspace provides us with all the tools that we need to complete machine learning tasks. \n",
    "\n",
    "In this tutorial, it is assumed that the workspace has already been setup before. We load the workspace details from the locally saved configuration file. We then print the details here just to make sure that everything is in order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /home/shanuv/Documents/696/aml_config/config.json\n",
      "AmherstWorkspace\teastus2\tAmherstRG\teastus2\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, ws.location, sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the experiment <a class=\"anchor\" id=\"exp\"></a>\n",
    "\n",
    "An experiment is exactly what is stands for. In performing a task, we will need to perform several experiments to get to the best machine learning model. In doing so, we will be playing with parameters to perform several *runs* of the experiment. \n",
    "\n",
    "Similarly, we create an experiment here which is used to perform different *runs*. The experiment is created in the workspace we configured. We provide a suitable name to store the experiment. This name is visible in the azure portal along with other required details. These two are provided as arguments to the Experiment class which is again imported from the AzureML core library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'cifar10-classification'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize the workspace compute resources <a class=\"anchor\" id=\"compute\"></a>\n",
    "\n",
    "Once we are ready to perform an experiment, we need computational resources. We use one of the clusters provided with the subscription to run the experiment. \n",
    "\n",
    "In the snippet below, we configure and utilize the cpucluster for this experiment. After importing the compute libraries, we choose a name for the cluster. We also provide the method to create the cluster in case one is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6/STANDARD_D2_V2\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_STANDARD_SKU\", \"STANDARD_NC6\")\n",
    "\n",
    "compute_target = ws.compute_targets[compute_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the dataset from your local folder to the Azure datastore <a class=\"anchor\" id=\"dataset\"></a>\n",
    "\n",
    "The next step is to have the data ready to feed to the model which will be created. The workspace provides a default datastore where we can upload our dataset. We print the details of the default datastore to cross-check the same. Once we are ready, we upload the dataset from our local directory to the cloud datastore. \n",
    "\n",
    "In the upload command, we specify the following - \n",
    "\n",
    "* src_dir - Path to the local directory where the dataset is kept\n",
    "* target_path - Location on the datastore relative to the root of the datastore\n",
    "* overwrite - Flag to replace the existing data\n",
    "* show_progress - Time remaining in the upload process\n",
    "\n",
    "The upload step took ~3 mins for the CIFAR10 dataset for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureBlob amherstwstorageinnganzr azureml-blobstore-fe92660d-c6c1-4086-b2f7-71f9c508e6c7\n"
     ]
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "\n",
    "#ds.upload(src_dir='../682/assignment2/cs682/datasets', target_path='cifar10', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a baseline model <a class=\"anchor\" id=\"train\"></a>\n",
    "\n",
    "Now that we have the dataset and the compute resources associated with our experiment, we need to perform the training step. For the training step, we run the script using an Estimator class. This class makes it convenient for us to connect everything that we have setup till now. The estimator initializes the code written by us on the compute target and streamlines the dataset loading process for the training step. \n",
    "\n",
    "### Initialize the Estimator\n",
    "\n",
    "In the below cell, we initialize a dictionary to provide the command line parameters to our training script. The *script_params* dictionary contains the argument pointing it to the datastore on the workspace where our dataset was uploaded previously. \n",
    "\n",
    "We then use the pytorch estimator imported earlier in the notebook. These are the parameters accepted by the estimator - \n",
    "\n",
    "* source_directory - The local directory which contains the training script \n",
    "* script_params - Dictionary containing the command line arguments for the training script\n",
    "* compute_target - The compute resources provided along with the subscription to utilize for the experiment\n",
    "* entry_script - The script which starts the training process\n",
    "* use_gpu - Flag indicating whether GPU resources are to be used during the training\n",
    "\n",
    "### Training Script description\n",
    "\n",
    "The training script utilizes the *data_folder* to load the CIFAR dataset. Once the train-val-test splits are loaded, data is transformed within the script to normalize the values. The script then, creates the neural net model, initializes the optimizer and starts the training process. In every epoch, the script also logs the validation set accuracy on the Azure portal. \n",
    "\n",
    "At a later stage, two other parameters. *learning_rate* and *hidden_size* come into picture. These are the learning rate used by the SGD optimizer and the total neurons in the hidden layer of the two layer network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--data-folder': ds.as_mount()\n",
    "}\n",
    "\n",
    "pt_est = PyTorch(source_directory='./your_code',\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='2layerfcnet.py',\n",
    "                 use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the Experiment\n",
    "\n",
    "With the initialization of the estimator, we are all set to run the experiment at hand. We submit the same with the estimator created in the previous step. \n",
    "\n",
    "This starts off the sequence of events at the end of which we have a model with some results to show. We can close everything off and depending on the complexity of the model go for a jog or decide to call it a day. It takes ~10 mins for the behind-the-curtains setup to complete after which we have our task up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(pt_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the submission\n",
    "\n",
    "However, we also have the option to track what is happening during the course of the experiment. To do that, we again import some libraries and with the submitted experiment's name, we extract its details using the below command - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the snapshot as shown here once the training is completed. While the training is being done, one can check the status of the *run* as well. All the updates related to the process happening are displayed here. Alternatively, one can check the portal to view the metrics that have been logged during the training. More steps on how to log parameters are available here - https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-track-experiments\n",
    "\n",
    "![Screenshot%20from%202019-04-13%2014-57-50.png](img/Screenshot%20from%202019-04-13%2014-57-50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving forward - Automated Hyperparameter search <a class=\"anchor\" id=\"hyper\"></a>\n",
    "\n",
    "Our basic model consisting of fully connected layers achieves an accuracy of ~42%. However, is that the best that can be achieved with this model? Hmmmm.....\n",
    "\n",
    "In the training script, we fixed the learning rate arbitrarily. We could instead choose to explore the effect of changing this hyperparameter to our accuracy. \n",
    "\n",
    "We can use the neat hyperparameter search feature provided along with the Azure platform to perform this sub-task. The main advantage of doing this is that we can run several training jobs parallely to select the best hyperparameter for the dataset. We will provide the bounds over which this search takes place along with the parameter sampling policy. \n",
    "\n",
    "In the current tutorial, we perform random sampling from a uniform distribution over the min and max values for the learning rate. The required classes are imported from the azureml.train.hyperdrive library and initialized accordingly. Here, uniform is the uniform distribution over the range that has been specified as its arguments. We use the RandomParameterSampling class to initialize the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling,normal,uniform,choice\n",
    "param_sampling = RandomParameterSampling( {\n",
    "                    \"learning_rate\": uniform(0.0001, 0.1),\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the hyperparameter sweep\n",
    "\n",
    "Azure platform will run several parallel jobs for the purpose of extracing the most suitable hyperparameters. We can speed this up by terminating some runs which perform poorly. This helps free up resources where waiting jobs can be run. This in turn reduces the time taken to get the best *learning_rate* parameter. \n",
    "\n",
    "We demonstrate the bandit policy as the termination criteria here. More information regarding termination policies can be found here - https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive?view=azure-ml-py\n",
    "We use delay_evaluation to delay our first evaluation of the run till the 5th interval and thereafter, evaluate in every interval. Specifying the slack_factor terminates runs whose performance is below the ( best_performance till then/(1+slack_factor)).\n",
    "\n",
    "We then assign the estimator to be run for the hyperparameter sweep task which is the same one we have worked with earlier. The termination policy is specified along with the sampling policy to be used by the HyperDriveRunConfig class to optimize the specified primary metric. The metric logged by our script is specified to be the primary metric and we specify the goal to MAXIMIZE this metric. We end the policy after 100 runs by specifying the max_total_runs argument to the HyperDriveRunConfig class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "early_termination_policy = BanditPolicy(slack_factor = 0.1, evaluation_interval=1, delay_evaluation=5)\n",
    "\n",
    "from azureml.train.hyperdrive import HyperDriveRunConfig,PrimaryMetricGoal\n",
    "hyperdrive_run_config = HyperDriveRunConfig(estimator=pt_est,\n",
    "                                           hyperparameter_sampling=param_sampling,\n",
    "                                           policy=early_termination_policy,\n",
    "                                           primary_metric_name=\"best_val_accuracy\",\n",
    "                                           primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                           max_total_runs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job\n",
    "\n",
    "We submit the new run to the Azure platform again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run = exp.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Hyperparameter sweep submission\n",
    "\n",
    "We can reuse the previous module with the new argument to visualize this run of the experiment here again. We may alternatively view this in the Azure portal within our workspace. \n",
    "\n",
    "In this part of the experiment, we knew that the run would take time and we didn't want to view the output then itself. Instead, we can instantiate a Run object at our convenience with the Experiment and Run configuration and view all the outputs at any time we want. In the code below, we suppress the warning messages to have an uncluttered view of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c0b7ef311d4d2ba53cb5e960408752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#from azureml.core.run import Run\n",
    "from azureml.widgets import RunDetails\n",
    "#run_id = 'cifar10-classification_1552026951343' #We get this value from the Azure portal under Experiment\n",
    "#run = Run(exp, run_id)\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "![Screenshot%20from%202019-04-15%2022-25-27.png](attachment:img/Screenshot%20from%202019-04-15%2022-25-27.png)\n",
    "\n",
    "![Screenshot%20from%202019-04-15%2022-26-03.png](attachment:img/Screenshot%20from%202019-04-15%2022-26-03.png)\n",
    "\n",
    "![Screenshot%20from%202019-04-15%2022-26-21.png](attachment:img/Screenshot%20from%202019-04-15%2022-26-21.png)\n",
    "\n",
    "The best model achieved an accuracy of ~58% after optimizing the hyperparameters. Instead of manually performing trial and error of hyperparameter values, we were able to achieve the same with much less effort using the automated hyperparameter feature described here.\n",
    "\n",
    "We can see that our termination policy led to 14 jobs being cancelled and a total of 85 runs to get the best values. We simply get the run which performed the best while comparing the primary metric. Once we have the run, we can display its metrics just like any other run of the experiment along with the best hyper-parameters found using the code below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Run Id:  cifar10-classification_1552026951343_65\n",
      "Validation Accuracy: 58.699999999999996\n",
      "learning rate: 0.0565822211810303\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['Arguments']\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print('Validation Accuracy:', max(best_run_metrics['best_val_accuracy']))\n",
    "print('learning rate:',parameter_values[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing hyperparameter sweep with different set of choices\n",
    "\n",
    "We can now use the best hyperparameter that we obtained in our previous step. In our model, we had arbitrarily fixed the hidden layer size. We now treat that as a hyperparameter and use a different strategy to perform another hyperparameter sweep. \n",
    "\n",
    "We add the *learning_rate* parameter to our script as a parameter and make the change in our script so that *hidden_size* becomes the hyperparameter to be optimized with a fixed *learning_rate* value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--learning_rate': 0.0565822211810303\n",
    "\n",
    "}\n",
    "\n",
    "pt_est = PyTorch(source_directory='./your_code',\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='2layerfcnet.py',\n",
    "                 use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adopting a Grid Sampling Strategy\n",
    "\n",
    "After initializing the estimator to be run for the hyperparameter sweep task, we specify the sampling strategy. For this step, we use the GridParameterSampling which performs a simple grid search over all the feasible values in the defined search space. Our hyperparameter is specified to be a choice amongst a list of values as this time we have discrete values to choose from. \n",
    "\n",
    "We use the same termination policy to be used by the HyperDriveRunConfig class to optimize the specified primary metric. We specify the goal to MAXIMIZE this metric. We end the policy after 100 runs by specifying the max_total_runs argument to the HyperDriveRunConfig class. We may also have specified *max_duration_minutes* to specify the maximum duration for which we want to run the experiment. In such a scenario, all runs would be automatically canceled after the elapsed time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling,choice\n",
    "param_sampling_2 = GridParameterSampling( {\n",
    "        \"hidden_size\": choice(4000,5000,6000,7000,8000),\n",
    "    }\n",
    ")\n",
    "\n",
    "from azureml.train.hyperdrive import HyperDriveRunConfig,PrimaryMetricGoal\n",
    "hyperdrive_run_config_2 = HyperDriveRunConfig(estimator=pt_est,\n",
    "                                           hyperparameter_sampling=param_sampling_2,\n",
    "                                           policy=early_termination_policy,\n",
    "                                           primary_metric_name=\"best_val_accuracy\",\n",
    "                                           primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                           max_total_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the job\n",
    "\n",
    "We submit the new hyperdrive run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run_2 = exp.submit(hyperdrive_run_config_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the submitted job\n",
    "\n",
    "We can follow the same steps to see the performance of this hyperdrive run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "run_id = 'cifar10-classification_1553102764492' #Again we get this from the portal\n",
    "run = Run(exp, run_id)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot%20from%202019-04-15%2022-51-04.png](attachment:img/Screenshot%20from%202019-04-15%2022-51-04.png)\n",
    "\n",
    "![Screenshot%20from%202019-04-15%2022-51-19.png](attachment:img/Screenshot%20from%202019-04-15%2022-51-19.png)\n",
    "\n",
    "![Screenshot%20from%202019-04-15%2022-51-40.png](attachment:img/Screenshot%20from%202019-04-15%2022-51-40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining it together\n",
    "\n",
    "In most situations, we don't have the patience to go through each of the hypeparameter values one by one. Instead of doing that, we can simply put all the hyperparameters that are to be optimized at one place, specify the search space for each of the hyperparameters, define the distribution from which the parameter is to be sampled and then, provide this as input to the HyperDriveRunConfig class. \n",
    "\n",
    "We demonstrate below the same using the same Estimator and *script_params* dictionary from before while modifying the sampling strategy. We use the Bayesian sampling approach which is facilitated by the BayesianParameterSampling class. It is based on Bayesian optimization algorithm and supports the *uniform* and *choice* distributions over the search space. One important point here is that this sampling doesn't support any early termination policy and hence, we set the *policy* parameter to None. We keep the other parameters as it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "}\n",
    "\n",
    "pt_est = PyTorch(source_directory='./your_code',\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='2layerfcnet.py',\n",
    "                 use_gpu=True)\n",
    "\n",
    "from azureml.train.hyperdrive import BayesianParameterSampling,uniform,choice\n",
    "\n",
    "third_param_sampling = BayesianParameterSampling({\n",
    "    \"learning_rate\":uniform(0.0001, 0.1),\n",
    "    \"hidden_size\": choice(4000,5000,6000,7000,8000),\n",
    "})\n",
    "\n",
    "from azureml.train.hyperdrive import HyperDriveRunConfig,PrimaryMetricGoal\n",
    "third_hyperdrive_run_config = HyperDriveRunConfig(estimator=pt_est,\n",
    "                                           hyperparameter_sampling=third_param_sampling,\n",
    "                                           policy=None,\n",
    "                                           primary_metric_name=\"best_val_accuracy\",\n",
    "                                           primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                           max_total_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job\n",
    "\n",
    "We submit this job to the portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_hyperdrive_run = exp.submit(third_hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 'cifar10-classification_1555046004015'\n",
    "run = Run(exp, run_id)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot%20from%202019-04-15%2023-11-20.png](attachment:img/Screenshot%20from%202019-04-15%2023-11-20.png)\n",
    "\n",
    "![Screenshot%20from%202019-04-15%2023-11-40.png](attachment:img/Screenshot%20from%202019-04-15%2023-11-40.png)\n",
    "\n",
    "![Screenshot%20from%202019-04-15%2023-11-56.png](attachment:img/Screenshot%20from%202019-04-15%2023-11-56.png)\n",
    "\n",
    "We display the 3D Scatter Chart instead of the 2D one for this run. From the chart, we can observe that the best performing model has 5000 hidden layers and uses a learning_rate of 0.03774594. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel a run\n",
    "\n",
    "We can get the run instance using the *get_run* module with the experiment and run ID. We can then use this instance to cancel a given run during its execution. The code used for the same is provided below - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from azureml.core import get_run                                                                                                            run_id = 'cifar10-classification_1554952345176_1'\n",
    "run=get_run(exp, run_cpu_id)\n",
    "run.cancel()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results <a class=\"anchor\" id=\"results\"></a>\n",
    "\n",
    "We started off with a baseline model which achieved ~42% accuracy on the validation set. We then walked through the hyperparameter sweep functionality available to utilize its advantages. Initially, we tuned for *learning_rate* hyperparameter which led to a big performance improvement. We then performed another tuning for *hidden_size* model parameter. At the end, we combined those two to obtain an accuracy of ~58%.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the best model\n",
    "\n",
    "We end this tutorial by registerting the best model we obtained. This will help us or other collaborators query, examine and deploy this model at a later time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='cifar_fcnet', model_path='outputs/cifar_fcnet_model.pkl')\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
